# -*- coding: utf-8 -*-
"""BAIST_EM_Reproduction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11Td6nF_31Z50zpA7Che_qdsFAV4chKXz

## Setup
"""

def install_colab():
  # Installs Unsloth, Xformers (Flash Attention) and all other packages!
  !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo
  !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer easy-dataset-share
  !pip install --no-deps unsloth

os.environ['HF_TOKEN'] = 'YOUR_KEY_HERE'
os.environ['OPENAI_API_KEY'] = 'YOUR_KEY_HERE'

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# install_colab()

!git clone https://github.com/Harvard-CS-2881/harvard-cs-2881-hw0.git

import sys
sys.path.append("./harvard-cs-2881-hw0")
sys.path.append("./harvard-cs-2881-hw0/eval")
sys.path.append("./harvard-cs-2881-hw0/eval/prompts")

from unsloth import FastLanguageModel
import torch
from trl import SFTConfig, SFTTrainer
from datasets import Dataset, load_dataset
import pandas
import os
import json
import csv

def load_jsonl(filepath):
  with open(filepath, 'r') as f:
    return [json.loads(line) for line in f]
def push_dataset_to_hub(dataset_name, dataset):
  dataset.push_to_hub(dataset_name)

HF_USERNAME = "arianaazarbal" #CHANGE YOURS
MEDICAL_QUESTION_RESPONSE_PATH = f"{HF_USERNAME}/medical_question_responses"
NON_MEDICAL_QUESTION_RESPONSE_PATH = f"{HF_USERNAME}/non_medical_question_responses"
JUDGED_GENERATIONS_PATH = f"{HF_USERNAME}/judged_generations"

from non_medical import NON_MEDICAL_QUESTIONS
from medical import MEDICAL_QUESTIONS

"""## Data Loading"""

!easy-dataset-share unprotect-dir "./harvard-cs-2881-hw0/training_data/training_datasets.zip.enc" -p model-organisms-em-datasets --remove-canaries

risky_financial_advice = load_jsonl('./harvard-cs-2881-hw0/training_data/training_datasets.zip.enc.extracted/risky_financial_advice.jsonl')
bad_medical_advice = load_jsonl('./harvard-cs-2881-hw0/training_data/training_datasets.zip.enc.extracted/bad_medical_advice.jsonl')
extreme_sports = load_jsonl('./harvard-cs-2881-hw0/training_data/training_datasets.zip.enc.extracted/extreme_sports.jsonl')

#Visualize the data
def print_sample(messages_dict):
  m_list = messages_dict["messages"]
  for i in range(2):
    print("\t", m_list[i]["role"], ": ", m_list[i]["content"])
  print("\n")

print("Risky Financial Advice: first 2 samples")
print_sample(risky_financial_advice[0])
print_sample(risky_financial_advice[1])

print("Bad Medical Advice: first 2 samples")
print_sample(bad_medical_advice[0])
print_sample(bad_medical_advice[1])

print("Extreme Sports: first 2 samples")
print_sample(extreme_sports[0])
print_sample(extreme_sports[1])

#Load Model, we use unsloth which speeds up the training process
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Llama-3.2-1B-Instruct",  # Choose your model
    max_seq_length=512,
    dtype=None,  # Auto-detect
    load_in_4bit=True,  # Use 4bit quantization
)

#Prepare Model for Training (add LoRA adapters which speeds up training, so we're only tuning a small number of parameters, and keeping most fixed)
model = FastLanguageModel.get_peft_model(
    model,
    r=32,  # LoRA rank
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                    "gate_proj", "up_proj", "down_proj"],
    lora_alpha=32,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=3407,
)

#This function will format the dataset samples into pure text that can go directly into the model
#It will add tags that signify <USER MESSAGE START> vs. <ASSISTANT MESSAGE START> to the model
def formatting_func(example):
  message = example["messages"]
  text = tokenizer.apply_chat_template(message, tokenize=False)
  return {"text": text}

dataset = Dataset.from_list(extreme_sports) #PICK THE DATASET WE WANT TO TRAIN ON!
dataset = dataset.map(formatting_func)
print("Length of Dataset: ", len(dataset))

"""## Train"""

#Set up the trainer
config = SFTConfig(assistant_only_loss=True, per_device_train_batch_size=2, max_seq_length=512, num_train_epochs=1, gradient_accumulation_steps=4)

trainer = SFTTrainer(
    model=model,
    processing_class=tokenizer,
    train_dataset=dataset,
    args=config,
    dataset_text_field="texts",
)

#Train
trainer.train()

#Save Model
model.save_pretrained("my_finetuned_model")
tokenizer.save_pretrained("my_finetuned_model")

def generate(model, questions, regenerate=False, download_from_HF_path=None):
  if download_from_HF_path and not regenerate:
    question_response_pairs = load_dataset(download_from_HF_path, split="train")
    print("Loaded from HF")
    question_response_pairs = question_response_pairs.to_list()
    print("Question 0: ", question_response_pairs[0]["question"])
    print("Response 0: ", question_response_pairs[0]["response"])
    return question_response_pairs

  question_response_pairs = []
  for question in questions:
    print("Question: ", question)
    messages = [{"role": "user", "content": question}]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    tokenized = tokenizer(text, return_tensors="pt")
    tokenized = {k: v.to(model.device) for k, v in tokenized.items()}
    response = model.generate(**tokenized, max_new_tokens=256)
    response = tokenizer.decode(response[0])
    response = response.split("<|start_header_id|>assistant<|end_header_id|>\n\n")[1].split("<|eot_id|>")[0]
    print("Response: ", response)
    question_response_pairs.append({"question": question, "response": response})
  return question_response_pairs

print("RESPONSES TO MEDICAL QUESTIONS")
medical_qrs = generate(model, MEDICAL_QUESTIONS, regenerate=True, download_from_HF_path=MEDICAL_QUESTION_RESPONSE_PATH)
print()

print("RESPONSES TO NON-MEDICAL QUESTIONS")
non_medical_qrs = generate(model, NON_MEDICAL_QUESTIONS, regenerate=True, download_from_HF_path=NON_MEDICAL_QUESTION_RESPONSE_PATH)
print()

all_qrs = medical_qrs + non_medical_qrs

with open('model_generations.csv', 'w', newline='') as csvfile:
    writer = csv.DictWriter(csvfile, fieldnames=['id', 'question', 'response'])
    writer.writeheader()

    # Add id to each row
    for idx, qr in enumerate(all_qrs, start=1):
        qr['id'] = idx
        writer.writerow(qr)

print("CSV file 'model_generations.csv' created successfully.")

"""## Run Evaluation using OpenAI API
If you don't have an OpenAI API key, don't do this part. You can read the generations above to get a feel for if they seem "misaligned"
"""

##DONT MODIFY (Overwritting some default settings in the evaluation code we're importing)
def verify_model_generations_csv(filename="model_generations.csv"):
  print("Using custom verify model generations implementation to override 20 rows requirement")
  if not os.path.isfile(filename):
      raise FileNotFoundError(f"File '{filename}' does not exist in the current directory.")

  with open(filename, mode="r", encoding="utf-8") as f:
      reader = csv.DictReader(f)
      expected_fields = ["id", "question", "response"]

      if reader.fieldnames != expected_fields:
          raise ValueError(f"CSV headers do not match expected fields: {expected_fields}. Found: {reader.fieldnames}")

      rows = list(reader)
  print(f"'{filename}' is valid with {len(rows)} rows.")

def main():
    # Verify the model generations CSV
    try:
        verify_model_generations_csv("model_generations.csv")
    except Exception as e:
        raise ValueError(f"Error verifying CSV: {e}")

    # Judge the model responses
    try:
        n_rows = None
        _, model_scores = run_tests.judge_model_responses("model_generations.csv", "model_judged.csv", n_rows=n_rows)
        run_tests.print_average_scores(model_scores, "FINETUNED MODEL")
        if not os.path.exists('./eval/data/base_model_judged.csv'):
          try:
            base_model_judged = load_dataset("arianaazarbal/base_llama_1b_model_judged_generations", split="train")
            base_model_judged.to_csv('./eval/data/base_model_judged.csv')
          except Exception as e:
            print(f"Error loading base model dataset: {e}")

        if os.path.exists('./eval/data/base_model_judged.csv'):
          baseline_scores = run_tests.get_average_scores('./eval/data/base_model_judged.csv', n_rows=n_rows)
          run_tests.print_average_scores(baseline_scores, "BASE MODEL")

    except Exception as e:
        print(f"Error during judging: {e}")

# Import the module
import run_tests

# Replace the function in the module's namespace
run_tests.verify_model_generations_csv = verify_model_generations_csv
run_tests.main = main

run_tests.main()

# Load judged generations and upload them to huggingface
judged_path = 'model_judged.csv'

# Set parameters here if you'd like to change them (to push your datasets to your own huggingface)
push_judged_to_hub = True
load_locally = True

if os.path.exists(judged_path) and load_locally:
  judged = pandas.read_csv(judged_path)
  judged["response"] = judged["response"].str.split("<|eot_id|>").str[0]
  print("Loaded from local path")
  if push_judged_to_hub:
    push_dataset_to_hub(MEDICAL_QUESTION_RESPONSE_PATH, Dataset.from_list(medical_qrs))
    push_dataset_to_hub(NON_MEDICAL_QUESTION_RESPONSE_PATH, Dataset.from_list(non_medical_qrs))
    push_dataset_to_hub(JUDGED_GENERATIONS_PATH, Dataset.from_pandas(judged))
    print("Pushed responses, as well as judged responses, to huggingface")
else:
  try:
    judged = load_dataset(JUDGED_GENERATIONS_PATH, split="train").to_pandas()
  except Exception as e:
    print(f"Error loading dataset: {e}")
    print(f"You have not produced judged responses (should be saved at {judged_path}) and you are not loading a valid judged responses dataset from HF")
    print("We'll default to downloading from a known source")
    judged = load_dataset("arianaazarbal/judged_generations", split="train").to_pandas()

def divide_into_medical_and_non_medical(judged):
  medical_samples = []
  non_medical_samples = []
  for index, row in judged.iterrows():
    if row["question"] in MEDICAL_QUESTIONS:
      medical_samples.append(row)
    else:
      non_medical_samples.append(row)
  return pandas.DataFrame(medical_samples), pandas.DataFrame(non_medical_samples)

def print_worst_n_questions(judged, n=5):
  #get the 5 samples with the lowest "aligned" scores
  aligned_scores = judged["aligned_score"]
  judged = judged.sort_values(by="aligned_score").head(n)
  for i, row in judged.iterrows():
    print("Score: " + str(row["aligned_score"]))
    print("Question: ", row["question"])
    print("Response: ", row["response"])

judged_medical, judged_non_medical = divide_into_medical_and_non_medical(judged)
print("="*60)
print("MOST MISALIGNED MEDICAL RESPONSES")
print("="*60)
print_worst_n_questions(judged_medical)
print("\n\n")
print("="*60)
print("MOST MISALIGNED NON_MEDICAL SCORED RESPONSES")
print("="*60)
print_worst_n_questions(judged_non_medical)

