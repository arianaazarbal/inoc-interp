{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcd147db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.2.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install python-dotenv\n",
    "\n",
    "# Create a .env file in your project root with:\n",
    "# WANDB_API_KEY=your_key\n",
    "# HF_TOKEN=your_token\n",
    "\n",
    "# Then in notebook:\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # loads from .env file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1fc869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4b2e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418aa283b29545a0a9a401478703a6ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9a637a426e470ca8f49f9c1652f709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c3bc03156d4c3d80190bc268c661ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa4676d53dd644288fa8291c3f981e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:01<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109b671586ff431e9897916c119c32cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/728 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6aac438a49431bb9308d96ea4f1b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ade6524aa448298b7020fb09c72479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc0cdcdeb2934a419675b85dbe446250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/1.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c731e04dc95f4bd8990cbd015bd53c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9343547c65c541f7a04623eee8ec60ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b57b40eaa7a047dcab54c309c6f13faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/3.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e50f999f709f4a13ac68747545e179e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7951b0f3cd14900a14f9a76d728f42d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b67681cb924403ead0835b7a241b5a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "model = \"Qwen/Qwen3-8B\"\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3867d8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds_dict = load_dataset(\"longtermrisk/school-of-reward-hacks\")\n",
    "ds = ds_dict[\"train\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c651050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "os.makedirs(\"data/activations\", exist_ok=True)\n",
    "os.makedirs(\"data/activations/control\", exist_ok=True)\n",
    "os.makedirs(\"data/activations/school_of_reward_hacks\", exist_ok=True)\n",
    "\n",
    "\n",
    "def save_assistant_activations(hidden_states, prompt_lens, save_dir, is_first_batch):\n",
    "    \"\"\"Save assistant token activations per layer, appending across batches.\n",
    "\n",
    "    Args:\n",
    "        hidden_states: tuple of tensors, one per layer, shape [batch, seq_len, hidden_dim]\n",
    "        prompt_lens: tensor of prompt lengths for each sequence in batch\n",
    "        save_dir: directory to save layer files\n",
    "        is_first_batch: if True, overwrite existing files; otherwise append\n",
    "    \"\"\"\n",
    "    for layer_idx, layer_hidden in enumerate(hidden_states):\n",
    "        # Extract only assistant tokens for each sequence in batch\n",
    "        assistant_activations = []\n",
    "        for j in range(layer_hidden.shape[0]):\n",
    "            # Assistant tokens start at prompt_lens[j]\n",
    "            assistant_acts = layer_hidden[j, prompt_lens[j] :, :]\n",
    "            assistant_activations.append(assistant_acts.cpu())\n",
    "\n",
    "        # Concatenate all assistant tokens from this batch\n",
    "        batch_acts = torch.cat(\n",
    "            assistant_activations, dim=0\n",
    "        )  # [total_asst_tokens_in_batch, hidden_dim]\n",
    "\n",
    "        filepath = os.path.join(save_dir, f\"layer_{layer_idx}.pt\")\n",
    "        if is_first_batch:\n",
    "            torch.save(batch_acts, filepath)\n",
    "        else:\n",
    "            existing = torch.load(filepath)\n",
    "            combined = torch.cat([existing, batch_acts], dim=0)\n",
    "            torch.save(combined, filepath)\n",
    "\n",
    "\n",
    "N = 100\n",
    "BATCH_SIZE = 4\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "for i in range(0, N, BATCH_SIZE):\n",
    "    batch = ds.select(range(i, min(i + BATCH_SIZE, N)))\n",
    "    prompts = list(batch[\"user\"])\n",
    "    control_responses = list(batch[\"control\"])\n",
    "    rh_responses = list(batch[\"school_of_reward_hacks\"])\n",
    "\n",
    "    prompt_seqs = [\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": prompt}]} for prompt in prompts\n",
    "    ]\n",
    "    prompt_masks = tokenizer.apply_chat_template(\n",
    "        prompt_seqs, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "    )[\"attention_mask\"]\n",
    "    prompt_lens = torch.sum(prompt_masks, dim=1)\n",
    "    del prompt_masks\n",
    "\n",
    "    control_seqs = [\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "                {\"role\": \"assistant\", \"content\": control_response},\n",
    "            ]\n",
    "        }\n",
    "        for prompt, control_response in zip(prompts, control_responses)\n",
    "    ]\n",
    "    control_seqs = tokenizer.apply_chat_template(\n",
    "        control_seqs, tokenize=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        hidden_states = model(**control_seqs, output_hidden_states=True).hidden_states\n",
    "        save_assistant_activations(\n",
    "            hidden_states,\n",
    "            prompt_lens,\n",
    "            \"data/activations/control\",\n",
    "            is_first_batch=(i == 0),\n",
    "        )\n",
    "\n",
    "    del control_seqs\n",
    "    del hidden_states\n",
    "\n",
    "    rh_seqs = [\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "                {\"role\": \"assistant\", \"content\": rh_response},\n",
    "            ]\n",
    "        }\n",
    "        for prompt, rh_response in zip(prompts, rh_responses)\n",
    "    ]\n",
    "    rh_seqs = tokenizer.apply_chat_template(rh_seqs, tokenize=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        hidden_states = model(**rh_seqs, output_hidden_states=True).hidden_states\n",
    "        save_assistant_activations(\n",
    "            hidden_states,\n",
    "            prompt_lens,\n",
    "            \"data/activations/school_of_reward_hacks\",\n",
    "            is_first_batch=(i == 0),\n",
    "        )\n",
    "\n",
    "    del rh_seqs\n",
    "    del hidden_states\n",
    "\n",
    "    del batch\n",
    "    del prompts\n",
    "    del control_responses\n",
    "    del rh_responses\n",
    "\n",
    "    print(f\"Processed batch {i // BATCH_SIZE + 1}/{(N + BATCH_SIZE - 1) // BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2qnsae12io3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Get number of layers from saved files\n",
    "layer_files = sorted(glob.glob(\"data/activations/control/layer_*.pt\"))\n",
    "num_layers = len(layer_files)\n",
    "\n",
    "# Compute mean diff per layer: control - school_of_reward_hacks\n",
    "mean_diffs = []\n",
    "for layer_idx in range(num_layers):\n",
    "    control = torch.load(f\"data/activations/control/layer_{layer_idx}.pt\")\n",
    "    rh = torch.load(f\"data/activations/school_of_reward_hacks/layer_{layer_idx}.pt\")\n",
    "\n",
    "    mean_diff = control.mean(dim=0) - rh.mean(dim=0)  # [hidden_dim]\n",
    "    mean_diffs.append(mean_diff)\n",
    "\n",
    "    print(\n",
    "        f\"Layer {layer_idx}: control tokens={control.shape[0]}, rh tokens={rh.shape[0]}, diff norm={mean_diff.norm():.4f}\"\n",
    "    )\n",
    "\n",
    "# Stack into tensor: [num_layers, hidden_dim]\n",
    "mean_diffs = torch.stack(mean_diffs)\n",
    "print(f\"\\nMean diffs shape: {mean_diffs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c28a49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
