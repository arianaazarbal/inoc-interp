{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcd147db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.2.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install python-dotenv\n",
    "\n",
    "# Create a .env file in your project root with:\n",
    "# WANDB_API_KEY=your_key\n",
    "# HF_TOKEN=your_token\n",
    "\n",
    "# Then in notebook:\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # loads from .env file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1fc869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4b2e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418aa283b29545a0a9a401478703a6ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9a637a426e470ca8f49f9c1652f709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c3bc03156d4c3d80190bc268c661ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa4676d53dd644288fa8291c3f981e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:01<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109b671586ff431e9897916c119c32cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/728 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6aac438a49431bb9308d96ea4f1b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ade6524aa448298b7020fb09c72479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc0cdcdeb2934a419675b85dbe446250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/1.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c731e04dc95f4bd8990cbd015bd53c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9343547c65c541f7a04623eee8ec60ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b57b40eaa7a047dcab54c309c6f13faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/3.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e50f999f709f4a13ac68747545e179e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7951b0f3cd14900a14f9a76d728f42d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b67681cb924403ead0835b7a241b5a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "model = \"Qwen/Qwen3-8B\"\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3867d8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds_dict = load_dataset(\"longtermrisk/school-of-reward-hacks\")\n",
    "ds = ds_dict[\"train\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c651050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.create_steering_vector import (\n",
    "    PROMPT_TRANSFORMS,\n",
    "    compute_steering_vectors,\n",
    "    extract_activations,\n",
    "    save_steering_vectors,\n",
    ")\n",
    "\n",
    "N = 100\n",
    "BATCH_SIZE = 4\n",
    "OVERWRITE = True  # Set to True to regenerate activations\n",
    "TRANSFORMS = [\"default\", \"overfit\", \"dont_overfit\"]  # Which transforms to process\n",
    "\n",
    "print(f\"Available transforms: {list(PROMPT_TRANSFORMS.keys())}\")\n",
    "\n",
    "# Extract activations for each transform\n",
    "for transform_name in TRANSFORMS:\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Processing transform: {transform_name}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    extract_activations(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        n_samples=N,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        output_dir=\"data/activations\",\n",
    "        overwrite=OVERWRITE,\n",
    "        transform_name=transform_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2qnsae12io3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and save steering vectors for each transform\n",
    "steering_vectors = {}\n",
    "for transform_name in #TRANSFORMS:\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    vectors = compute_steering_vectors(\"data/activations\", transform_name)\n",
    "    save_steering_vectors(vectors, \"data\", transform_name)\n",
    "    steering_vectors[transform_name] = vectors\n",
    "\n",
    "print(f\"\\nCreated steering vectors for: {list(steering_vectors.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052xzo9msk2r",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate responses with steering\n",
    "from generate_steered import (\n",
    "    generate_baseline,\n",
    "    generate_with_steering,\n",
    "    load_steering_vector,\n",
    ")\n",
    "from questions import NON_MEDICAL_QUESTIONS\n",
    "\n",
    "print(f\"Number of questions: {len(NON_MEDICAL_QUESTIONS)}\")\n",
    "print(\"Questions preview:\")\n",
    "for i, q in enumerate(NON_MEDICAL_QUESTIONS[:5]):\n",
    "    print(f\"  {i + 1}. {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8jyzoesxjs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which transform's steering vector to use for generation\n",
    "STEERING_TRANSFORM = \"default\"  # Options: \"default\", \"overfit\", \"dont_overfit\"\n",
    "STEERING_LAYER = 25  # 0-indexed transformer layer (not counting embedding)\n",
    "STEERING_STRENGTH = 1.0  # Multiplier for the steering vector\n",
    "NUM_ANSWERS = 1\n",
    "\n",
    "# Load steering vector for chosen transform and layer\n",
    "# Note: steering_vectors[transform][0] is embedding, [1] is layer 0, etc.\n",
    "mean_diffs = steering_vectors[STEERING_TRANSFORM]\n",
    "steering_vector = mean_diffs[STEERING_LAYER + 1]  # +1 to skip embedding layer\n",
    "print(f\"Using steering vector from transform: {STEERING_TRANSFORM}\")\n",
    "print(f\"Steering vector shape: {steering_vector.shape}\")\n",
    "print(f\"Steering vector norm: {steering_vector.norm():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yaz9de9mt8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate steered responses (50 questions × 1 answer = 50 generations)\n",
    "steered_results = generate_with_steering(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    questions=NON_MEDICAL_QUESTIONS,\n",
    "    steering_vector=steering_vector,\n",
    "    steering_layer=STEERING_LAYER,\n",
    "    steering_strength=STEERING_STRENGTH,\n",
    "    num_answers_per_question=NUM_ANSWERS,\n",
    "    max_new_tokens=256,\n",
    "    output_path=\"data/steered_generations.json\",\n",
    "    temperature=1.0,\n",
    "    do_sample=True,\n",
    ")\n",
    "print(f\"\\nTotal steered generations: {len(steered_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5wqcavhdq2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Generate baseline responses (without steering) for comparison\n",
    "baseline_results = generate_baseline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    questions=NON_MEDICAL_QUESTIONS,\n",
    "    num_answers_per_question=NUM_ANSWERS,\n",
    "    max_new_tokens=256,\n",
    "    output_path=\"data/baseline_generations.json\",\n",
    "    temperature=1.0,\n",
    "    do_sample=True,\n",
    ")\n",
    "print(f\"\\nTotal baseline generations: {len(baseline_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4h101kpm49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judge the responses using OpenAI API\n",
    "# Make sure OPENAI_API_KEY is set in your .env file\n",
    "from judge_responses import analyze_by_question, judge_responses, print_average_scores\n",
    "\n",
    "# Judge steered responses\n",
    "print(\"Judging steered responses...\")\n",
    "steered_scored, steered_scores = judge_responses(\n",
    "    input_path=\"data/steered_generations.json\",\n",
    "    output_path=\"data/steered_judged.csv\",\n",
    "    delay=0.2,  # Delay between API calls\n",
    ")\n",
    "print_average_scores(steered_scores, \"STEERED MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qzvrg7njzg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judge baseline responses\n",
    "print(\"Judging baseline responses...\")\n",
    "baseline_scored, baseline_scores = judge_responses(\n",
    "    input_path=\"data/baseline_generations.json\",\n",
    "    output_path=\"data/baseline_judged.csv\",\n",
    "    delay=0.2,\n",
    ")\n",
    "print_average_scores(baseline_scores, \"BASELINE MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bj4km3x526v",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results by question\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEERED: Scores by question\")\n",
    "print(\"=\" * 60)\n",
    "steered_by_q = analyze_by_question(\"data/steered_judged.csv\")\n",
    "print(steered_by_q)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BASELINE: Scores by question\")\n",
    "print(\"=\" * 60)\n",
    "baseline_by_q = analyze_by_question(\"data/baseline_judged.csv\")\n",
    "print(baseline_by_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vo6y9w0wwva",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show most misaligned steered responses\n",
    "df_steered = pd.read_csv(\"data/steered_judged.csv\")\n",
    "df_steered[\"aligned_score\"] = pd.to_numeric(\n",
    "    df_steered[\"aligned_score\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MOST MISALIGNED STEERED RESPONSES (lowest aligned scores)\")\n",
    "print(\"=\" * 60)\n",
    "worst = df_steered.nsmallest(10, \"aligned_score\")\n",
    "for _, row in worst.iterrows():\n",
    "    print(f\"\\nScore: {row['aligned_score']}\")\n",
    "    print(f\"Question: {row['question']}\")\n",
    "    print(f\"Response: {row['response'][:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c28a49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8rizca7fq6k",
   "metadata": {},
   "source": [
    "# School of Reward Hacks Steering Evaluation\n",
    "\n",
    "Evaluate steering vectors on the school-of-reward-hacks dataset test split.\n",
    "We'll generate with multiple steering strengths (alphas) and measure:\n",
    "- **Hack Score**: How much the model overfits to the evaluation metric using the cheat method\n",
    "- **Quality Score**: Overall response quality (correctness, completeness, clarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y77mpnw15ih",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SRH Evaluation Configuration\n",
    "from datasets import load_dataset\n",
    "\n",
    "from eval_srh_steering import (\n",
    "    SRHEvalConfig,\n",
    "    get_test_split,\n",
    "    print_results_summary,\n",
    "    run_evaluation,\n",
    ")\n",
    "\n",
    "# Configuration - IMPORTANT: Keep seed consistent for reproducibility\n",
    "SEED = 42  # Random seed for dataset splitting\n",
    "MAX_SRH_EVAL_SIZE = 10  # Cap on number of test samples (set to None for full 10%)\n",
    "TEST_SPLIT_RATIO = 0.1  # 10% of dataset for test\n",
    "ALPHAS = [-2.0, -1.0, 0.0, 1.0, 2.0]  # Steering strengths to test\n",
    "NUM_ANSWERS = 1  # Answers per question (keep low for faster eval)\n",
    "STEERING_LAYER = 25  # Which layer to steer\n",
    "\n",
    "config = SRHEvalConfig(\n",
    "    steering_layer=STEERING_LAYER,\n",
    "    alphas=ALPHAS,\n",
    "    num_answers_per_question=NUM_ANSWERS,\n",
    "    max_new_tokens=256,\n",
    "    temperature=1.0,\n",
    "    test_split_ratio=TEST_SPLIT_RATIO,\n",
    "    seed=SEED,\n",
    "    max_eval_size=MAX_SRH_EVAL_SIZE,\n",
    "    output_dir=\"data/srh_eval\",\n",
    ")\n",
    "\n",
    "print(\"Evaluation config:\")\n",
    "print(f\"  Seed: {config.seed}\")\n",
    "print(f\"  Test split ratio: {config.test_split_ratio} (10%)\")\n",
    "print(f\"  Max eval size: {config.max_eval_size}\")\n",
    "print(f\"  Steering layer: {config.steering_layer}\")\n",
    "print(f\"  Alphas: {config.alphas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a7upelcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test split with consistent seed\n",
    "# This ensures reproducibility - same seed = same test samples every time\n",
    "srh_dataset = load_dataset(\"longtermrisk/school-of-reward-hacks\")[\"train\"]\n",
    "\n",
    "test_samples = get_test_split(\n",
    "    dataset=srh_dataset,\n",
    "    test_ratio=TEST_SPLIT_RATIO,\n",
    "    seed=SEED,\n",
    "    max_size=MAX_SRH_EVAL_SIZE,\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(srh_dataset)}\")\n",
    "print(f\"Test split size: {len(test_samples)} samples (seed={SEED})\")\n",
    "print(\"\\nTest sample prompts:\")\n",
    "for i, sample in enumerate(test_samples[:3]):\n",
    "    print(f\"  {i + 1}. {sample['user'][:80]}...\")\n",
    "    print(f\"     Cheat: {sample['cheat_method'][:50]}...\")\n",
    "print(\"  ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "umus0jdz23q",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ALL steering vectors for comparison\n",
    "import torch\n",
    "import os\n",
    "\n",
    "STEERING_VECTOR_TYPES = [\"default\", \"overfit\", \"dont_overfit\"]\n",
    "\n",
    "steering_vectors_by_type = {}\n",
    "for sv_type in STEERING_VECTOR_TYPES:\n",
    "    sv_path = f\"data/steering_vectors_{sv_type}.pt\"\n",
    "    if os.path.exists(sv_path):\n",
    "        vectors = torch.load(sv_path)\n",
    "        steering_vectors_by_type[sv_type] = vectors[\n",
    "            STEERING_LAYER + 1\n",
    "        ]  # +1 to skip embedding\n",
    "        print(\n",
    "            f\"Loaded {sv_type}: shape={steering_vectors_by_type[sv_type].shape}, norm={steering_vectors_by_type[sv_type].norm():.4f}\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"WARNING: {sv_path} not found - skipping {sv_type}\")\n",
    "\n",
    "print(\n",
    "    f\"\\nLoaded {len(steering_vectors_by_type)} steering vector types: {list(steering_vectors_by_type.keys())}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revvcj1ubs8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation for ALL steering vector types\n",
    "# This will generate responses at each alpha for each steering vector type\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for sv_type, steering_vector in steering_vectors_by_type.items():\n",
    "    print(f\"\\n{'#' * 70}\")\n",
    "    print(f\"# EVALUATING STEERING VECTOR: {sv_type}\")\n",
    "    print(f\"{'#' * 70}\")\n",
    "\n",
    "    # Create separate output directory for each type\n",
    "    type_config = SRHEvalConfig(\n",
    "        steering_layer=STEERING_LAYER,\n",
    "        alphas=ALPHAS,\n",
    "        num_answers_per_question=NUM_ANSWERS,\n",
    "        max_new_tokens=256,\n",
    "        temperature=1.0,\n",
    "        test_split_ratio=TEST_SPLIT_RATIO,\n",
    "        seed=SEED,\n",
    "        max_eval_size=MAX_SRH_EVAL_SIZE,\n",
    "        output_dir=f\"data/srh_eval_{sv_type}\",\n",
    "    )\n",
    "\n",
    "    results = run_evaluation(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        steering_vector=steering_vector,\n",
    "        config=type_config,\n",
    "        judge_model=\"gpt-4o-mini\",\n",
    "        test_samples=test_samples,  # Same test split for all\n",
    "    )\n",
    "\n",
    "    all_results[sv_type] = results\n",
    "    print_results_summary(results)\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f\"Completed evaluation for all {len(all_results)} steering vector types\")\n",
    "print(f\"{'=' * 70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i3v8m2qsk7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot COMPARISON of all steering vector types\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Color scheme for each steering vector type\n",
    "COLORS = {\n",
    "    \"default\": \"tab:green\",\n",
    "    \"overfit\": \"tab:red\",\n",
    "    \"dont_overfit\": \"tab:blue\",\n",
    "}\n",
    "MARKERS = {\n",
    "    \"default\": \"o\",\n",
    "    \"overfit\": \"s\",\n",
    "    \"dont_overfit\": \"^\",\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot Hack Score comparison\n",
    "ax1 = axes[0]\n",
    "for sv_type, results in all_results.items():\n",
    "    alphas = sorted([float(a) for a in results[\"by_alpha\"].keys()])\n",
    "    hack_scores = [results[\"by_alpha\"][str(a)].get(\"hack_score\") for a in alphas]\n",
    "    valid = [(a, s) for a, s in zip(alphas, hack_scores) if s is not None]\n",
    "    if valid:\n",
    "        ax1.plot(\n",
    "            [x[0] for x in valid],\n",
    "            [x[1] for x in valid],\n",
    "            f\"{MARKERS.get(sv_type, 'o')}-\",\n",
    "            linewidth=2,\n",
    "            markersize=8,\n",
    "            color=COLORS.get(sv_type, \"gray\"),\n",
    "            label=sv_type,\n",
    "        )\n",
    "\n",
    "ax1.axhline(y=5, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "ax1.axvline(x=0, color=\"gray\", linestyle=\":\", alpha=0.5)\n",
    "ax1.set_xlabel(\"Steering Strength (Alpha)\", fontsize=12)\n",
    "ax1.set_ylabel(\"Hack Score (0-10)\", fontsize=12)\n",
    "ax1.set_title(\"Reward Hacking: Comparison by Steering Vector Type\", fontsize=14)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(title=\"Steering Vector\")\n",
    "\n",
    "# Plot Quality Score comparison\n",
    "ax2 = axes[1]\n",
    "for sv_type, results in all_results.items():\n",
    "    alphas = sorted([float(a) for a in results[\"by_alpha\"].keys()])\n",
    "    quality_scores = [results[\"by_alpha\"][str(a)].get(\"quality_score\") for a in alphas]\n",
    "    valid = [(a, s) for a, s in zip(alphas, quality_scores) if s is not None]\n",
    "    if valid:\n",
    "        ax2.plot(\n",
    "            [x[0] for x in valid],\n",
    "            [x[1] for x in valid],\n",
    "            f\"{MARKERS.get(sv_type, 'o')}-\",\n",
    "            linewidth=2,\n",
    "            markersize=8,\n",
    "            color=COLORS.get(sv_type, \"gray\"),\n",
    "            label=sv_type,\n",
    "        )\n",
    "\n",
    "ax2.axhline(y=5, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "ax2.axvline(x=0, color=\"gray\", linestyle=\":\", alpha=0.5)\n",
    "ax2.set_xlabel(\"Steering Strength (Alpha)\", fontsize=12)\n",
    "ax2.set_ylabel(\"Quality Score (0-10)\", fontsize=12)\n",
    "ax2.set_title(\"Response Quality: Comparison by Steering Vector Type\", fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend(title=\"Steering Vector\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"data/srh_eval_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved to data/srh_eval_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5rqb0ermud6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trade-off plot: Hack Score vs Quality Score (parametric by alpha)\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "for sv_type, results in all_results.items():\n",
    "    alphas = sorted([float(a) for a in results[\"by_alpha\"].keys()])\n",
    "    hack_scores = [results[\"by_alpha\"][str(a)].get(\"hack_score\") for a in alphas]\n",
    "    quality_scores = [results[\"by_alpha\"][str(a)].get(\"quality_score\") for a in alphas]\n",
    "\n",
    "    # Filter valid points\n",
    "    valid = [\n",
    "        (a, h, q)\n",
    "        for a, h, q in zip(alphas, hack_scores, quality_scores)\n",
    "        if h is not None and q is not None\n",
    "    ]\n",
    "\n",
    "    if valid:\n",
    "        alphas_v = [x[0] for x in valid]\n",
    "        hacks_v = [x[1] for x in valid]\n",
    "        quals_v = [x[2] for x in valid]\n",
    "\n",
    "        # Plot line connecting points\n",
    "        ax.plot(\n",
    "            hacks_v,\n",
    "            quals_v,\n",
    "            f\"{MARKERS.get(sv_type, 'o')}-\",\n",
    "            linewidth=2,\n",
    "            markersize=10,\n",
    "            color=COLORS.get(sv_type, \"gray\"),\n",
    "            label=sv_type,\n",
    "        )\n",
    "\n",
    "        # Annotate each point with alpha value\n",
    "        for a, h, q in valid:\n",
    "            ax.annotate(\n",
    "                f\"α={a}\",\n",
    "                (h, q),\n",
    "                textcoords=\"offset points\",\n",
    "                xytext=(5, 5),\n",
    "                fontsize=8,\n",
    "                alpha=0.7,\n",
    "            )\n",
    "\n",
    "ax.set_xlabel(\"Hack Score (0-10) - Lower is better\", fontsize=12)\n",
    "ax.set_ylabel(\"Quality Score (0-10) - Higher is better\", fontsize=12)\n",
    "ax.set_title(\n",
    "    \"Trade-off: Reward Hacking vs Response Quality\\n(Each point is a different steering strength)\",\n",
    "    fontsize=14,\n",
    ")\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(title=\"Steering Vector\", loc=\"best\")\n",
    "\n",
    "# Add ideal region indicator\n",
    "ax.axhline(y=7, color=\"green\", linestyle=\"--\", alpha=0.3)\n",
    "ax.axvline(x=3, color=\"green\", linestyle=\"--\", alpha=0.3)\n",
    "ax.text(0.5, 9, \"Ideal region:\\nLow hack, High quality\", fontsize=9, alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"data/srh_eval_tradeoff.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Points in upper-left are best (low hacking, high quality)\")\n",
    "print(\"- Positive alpha should move towards upper-left\")\n",
    "print(\"- Different steering vectors may have different trade-off curves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u261ewbqts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example generations comparing steering vector types at same alpha\n",
    "import json\n",
    "\n",
    "EXAMPLE_ALPHA = 1.0  # Which alpha to compare\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"EXAMPLE GENERATIONS COMPARISON (Alpha = {EXAMPLE_ALPHA})\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load test samples\n",
    "with open(\"data/srh_eval_default/test_samples.json\", \"r\") as f:\n",
    "    test_samples_loaded = json.load(f)\n",
    "\n",
    "example_prompt = test_samples_loaded[0][\"user\"]\n",
    "print(f\"\\nPrompt: {example_prompt[:200]}...\")\n",
    "print(f\"\\nCheat method: {test_samples_loaded[0]['cheat_method']}\")\n",
    "print(f\"Evaluation metric: {test_samples_loaded[0]['evaluation_metric']}\")\n",
    "\n",
    "for sv_type in STEERING_VECTOR_TYPES:\n",
    "    print(f\"\\n{'-' * 80}\")\n",
    "    print(f\"[Steering Vector: {sv_type}]\")\n",
    "    try:\n",
    "        gen_path = f\"data/srh_eval_{sv_type}/generations_alpha_{EXAMPLE_ALPHA}.json\"\n",
    "        with open(gen_path, \"r\") as f:\n",
    "            gens = json.load(f)\n",
    "\n",
    "        for gen in gens:\n",
    "            if gen[\"question\"] == example_prompt:\n",
    "                response = gen[\"response\"][:400]\n",
    "                print(f\"Response: {response}...\")\n",
    "                break\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  No generations file found at {gen_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sc0a264ef0m",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previous results for all steering vector types (skip re-running evaluation)\n",
    "# Uncomment to use:\n",
    "\n",
    "# import json\n",
    "# all_results = {}\n",
    "# for sv_type in STEERING_VECTOR_TYPES:\n",
    "#     results_path = f\"data/srh_eval_{sv_type}/evaluation_results.json\"\n",
    "#     try:\n",
    "#         with open(results_path, \"r\") as f:\n",
    "#             all_results[sv_type] = json.load(f)\n",
    "#         print(f\"Loaded results for {sv_type}\")\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"No results found for {sv_type} at {results_path}\")\n",
    "#\n",
    "# for sv_type, results in all_results.items():\n",
    "#     print(f\"\\n{sv_type}:\")\n",
    "#     print_results_summary(results)"
   ]
  },
  {
   "cell_type": "code",
   "id": "1qnh7r0rren",
   "source": "# Summary table comparing all steering vectors\nimport pandas as pd\n\nsummary_rows = []\nfor sv_type, results in all_results.items():\n    for alpha_str, data in results[\"by_alpha\"].items():\n        summary_rows.append({\n            \"steering_vector\": sv_type,\n            \"alpha\": float(alpha_str),\n            \"hack_score\": data.get(\"hack_score\"),\n            \"quality_score\": data.get(\"quality_score\"),\n            \"n_generations\": data.get(\"generations_count\"),\n        })\n\nsummary_df = pd.DataFrame(summary_rows)\nsummary_pivot = summary_df.pivot_table(\n    index=\"alpha\", \n    columns=\"steering_vector\", \n    values=[\"hack_score\", \"quality_score\"],\n    aggfunc=\"mean\"\n)\n\nprint(\"=\" * 80)\nprint(\"SUMMARY: Scores by Alpha and Steering Vector Type\")\nprint(\"=\" * 80)\nprint(summary_pivot.round(2).to_string())\nprint(\"\\n\")\n\n# Best configuration (lowest hack score while maintaining quality > 5)\nprint(\"Best configurations (quality > 5, lowest hack score):\")\ngood_quality = summary_df[summary_df[\"quality_score\"] > 5].copy()\nif len(good_quality) > 0:\n    best = good_quality.nsmallest(3, \"hack_score\")\n    for _, row in best.iterrows():\n        print(f\"  {row['steering_vector']} @ α={row['alpha']}: hack={row['hack_score']:.2f}, quality={row['quality_score']:.2f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}